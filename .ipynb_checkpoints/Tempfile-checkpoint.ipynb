{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexdecomp_model (semantic matching + CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dot, Lambda, Conv2D\n",
    "from keras.layers import MaxPooling2D, Flatten, Concatenate, Dense\n",
    "from keras.layers import Activation, BatchNormalization, Dropout\n",
    "\n",
    "#####################################debut algo ###############################################\n",
    "\n",
    "def semantic_match(X, Y, A, window): #local-w \n",
    "    \"\"\"Computing semantic match in direction X -> Y\n",
    "    shape X: (s,n,d), Y: (s,m,d), A: (s, n, m)\n",
    "    A is the semantic matching at word-word level\n",
    "    window : w (=3)\n",
    "    \"\"\"\n",
    "    # shape Pivot, lower_lim, upper_lim: (s,n,1)\n",
    "    #Pivot = k : the max matching between each word of X and \n",
    "    #a whole sentence of Y for each sentence of X\n",
    "    #for each sentence of Y\n",
    "    \n",
    "    Pivot = np.expand_dims(np.argmax(A, axis=-1), axis=-1) \n",
    "    lower_lim = np.maximum(0, Pivot-window) #\n",
    "    upper_lim = np.minimum(A.shape[-1], Pivot+window)\n",
    "\n",
    "    # shape indices: (s,n,m)\n",
    "    # indices = np.tile(np.arange(A.shape[2]), (A.shape[0], A.shape[1] ,1))\n",
    "    indices = np.tile(np.arange(A.shape[-1]), A.shape[:-1]+(1,))\n",
    "    # NOTE: To replicate \"mcrisc\" implementation in github use: indices < upper_lim\n",
    "    mask = ((indices >= lower_lim) & (indices <= upper_lim)).astype(np.float32)\n",
    "\n",
    "    # shape X_hat: (n,d)\n",
    "    X_hat = np.matmul(A*mask, Y)\n",
    "\n",
    "    return X_hat #ligne 7 de l'algo \n",
    "\n",
    "def decompose(X, X_hat, method=\"linear\"):\n",
    "    \"\"\"Decompose a dataset into pos and neg components \n",
    "    with regards to its semantic match version\n",
    "    \n",
    "    shape X, X_hat: (s,n,d)\n",
    "    \"\"\"\n",
    "    assert method in (\"linear\", \"orthogonal\")\n",
    "    if method == \"linear\":\n",
    "        # shape alpha: (s,n,1)\n",
    "        denom = (np.linalg.norm(X, axis=-1, keepdims=True) *\n",
    "                 np.linalg.norm(X_hat, axis=-1, keepdims=True))\n",
    "        alpha = np.divide(np.sum(X * X_hat, axis=-1, keepdims=True),\n",
    "                          denom, where=denom!=0)\n",
    "\n",
    "        # shape X_pos, X_neg: (s,n,d)\n",
    "        X_pos = alpha * X\n",
    "        X_neg = (1 - alpha) * X\n",
    "    elif method == \"orthogonal\": #the chosen one (line 8)\n",
    "        # shape X_pos, X_neg: (s,n,d)\n",
    "        denom = np.sum(X_hat * X_hat, axis=-1, keepdims=True)\n",
    "        X_pos = np.divide(np.sum(X * X_hat, axis=-1, keepdims=True),\n",
    "                          denom, where=denom!=0) * X_hat\n",
    "        X_neg = X - X_pos\n",
    "    X_pos = np.expand_dims(X_pos, axis=-1)\n",
    "    X_neg = np.expand_dims(X_neg, axis=-1)\n",
    "    # shape X_decomp: (s,n,d,2)\n",
    "    X_decomp = np.concatenate([X_pos, X_neg], axis=-1)\n",
    "    return X_decomp\n",
    "\n",
    "\n",
    "def decompose_data(X, Y, window=3, method=\"linear\"): \n",
    "    \"\"\"Decompose datasets X, Y into positive and negative\n",
    "    channels with regards to each other\n",
    "    shape X: (s,n,d), Y: (s,m,d)\n",
    "    \"\"\"\n",
    "    # Cosine similarity\n",
    "    # shape A: (s,n,m)\n",
    "    norm_X = np.linalg.norm(X, axis=-1, keepdims=True)\n",
    "    norm_Y = np.linalg.norm(Y, axis=-1, keepdims=True)\n",
    "    A = np.matmul(np.divide(X, norm_X, where=norm_X!=0), np.swapaxes(np.divide(Y, norm_Y, where=norm_Y!=0), -1, -2))\n",
    "    A = np.matmul(np.divide(X, norm_X, where=norm_X!=0), np.swapaxes(np.divide(Y, norm_Y, where=norm_Y!=0), -1, -2))\n",
    "\n",
    "    # Semantic matching\n",
    "    # shape X_hat: (s,n,d), Y_hat: (s,m,d)\n",
    "    X_hat = semantic_match(X, Y, A, window=window)\n",
    "    Y_hat = semantic_match(Y, X, np.swapaxes(A, -1, -2), window=window)\n",
    "    # Decomposition (pos, neg)\n",
    "    X_decomp = decompose(X, X_hat, method=method)\n",
    "    Y_decomp = decompose(Y, Y_hat, method=method)\n",
    "\n",
    "    return X_decomp, Y_decomp #lines 8 and 12\n",
    "\n",
    "\n",
    "#####################################fin algo ###############################################\n",
    "\n",
    "\n",
    "def transform_data(X, embedding_matrix):\n",
    "    X_emb = np.zeros(X.shape+(embedding_matrix.shape[1],))\n",
    "    for i, val in np.ndenumerate(X):\n",
    "        X_emb[i] = embedding_matrix[val]\n",
    "    return X_emb\n",
    "\n",
    "\n",
    "def CNN_encoder(input_shape, embeddings_dim, max_seq_length, filters):\n",
    "    X_input = Input(input_shape)\n",
    "    # Applying different filter sizes at the same time\n",
    "    conv_list = []\n",
    "    for i, (filter_size, number_of_filters) in enumerate(filters):\n",
    "        # Convolutional layer\n",
    "        # Output shape: (batch_size, width_conv, number_of_filters)\n",
    "        conv = Conv2D(filters=number_of_filters,\n",
    "                      kernel_size=(filter_size, embeddings_dim),\n",
    "                      strides=1,\n",
    "                      padding=\"valid\",\n",
    "                      data_format=\"channels_last\",\n",
    "                      name=\"conv\"+str(i))(X_input)\n",
    "        #conv = BatchNormalization()(conv)\n",
    "        conv = Activation(\"tanh\")(conv)\n",
    "\n",
    "        # Max-pooling layer\n",
    "        # Output shape: (batch_size, 1, number_of_filters)\n",
    "        width_conv = max_seq_length - filter_size + 1\n",
    "        conv = MaxPooling2D(pool_size=(width_conv, 1),\n",
    "                            name=\"maxpool\"+str(i))(conv)\n",
    "        # Flattening because we only have one layer of conv filters\n",
    "        # Output shape: (batch_size, number_of_filters)\n",
    "        conv = Flatten()(conv)\n",
    "\n",
    "        # storing all conv filters\n",
    "        conv_list.append(conv)\n",
    "\n",
    "    # Concatenating the outputs of different filter sizes\n",
    "    if len(filters) > 1:\n",
    "        X = Concatenate()(conv_list)\n",
    "    else:\n",
    "        X = conv_list[0]\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X)\n",
    "    return model\n",
    "\n",
    "\n",
    "def lexdecomp_model(input_shape, embeddings_dim, max_seq_length, filters, dropout=0.5, model_type=\"other\"):\n",
    "    S_input = Input(input_shape)\n",
    "    T_input = Input(input_shape)\n",
    "\n",
    "    # Weight-sharing encoder (Siamese architecture)\n",
    "    if model_type == \"siamese\":\n",
    "        encoder = CNN_encoder(input_shape, embeddings_dim, max_seq_length, filters)\n",
    "        S_encoded = encoder(S_input)\n",
    "        T_encoded = encoder(T_input)\n",
    "    else:\n",
    "        S_encoded = CNN_encoder(input_shape, embeddings_dim, max_seq_length, filters)(S_input)\n",
    "        T_encoded = CNN_encoder(input_shape, embeddings_dim, max_seq_length, filters)(T_input)\n",
    "\n",
    "    X = Concatenate()([S_encoded, T_encoded])\n",
    "    X = Dropout(dropout)(X)\n",
    "    X = Dense(1, activation=\"sigmoid\")(X)\n",
    "\n",
    "    model = Model(inputs=[S_input, T_input], outputs=X, name=\"lexdecomp_model\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wang_lexcomp_approach (Training + test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, confusion_matrix\n",
    "import pickle\n",
    "import lexdecomp_model\n",
    "import utils\n",
    "import datetime\n",
    "\n",
    "###################\n",
    "# DATA PARAMETERS #\n",
    "###################\n",
    "version = \"20180427\"\n",
    "# myprep, kimprep\n",
    "pp_name = \"kimprep\"\n",
    "lower_opt = \"nolower\"\n",
    "emb_opt = \"embcap\"\n",
    "# word2vec, glove, paragram\n",
    "emb_name = \"word2vec\"\n",
    "# Duplicate training by switching pairs\n",
    "reverse_train=False\n",
    "# Randomly generate negative samples\n",
    "autoneg = 0\n",
    "#######################\n",
    "# END DATA PARAMETERS #\n",
    "#######################\n",
    "\n",
    "# Generating dataset from parsed MSRPC\n",
    "(index_to_word, word_to_index,\n",
    " X_train1, X_train2, Y_train,\n",
    " X_test1, X_test2, Y_test) = utils.generate_dataset(pp_name, lower_opt, version,\n",
    "                                      max_seq_length=-1,\n",
    "                                      reverse_train_pairs=reverse_train,\n",
    "                                      padding=True,\n",
    "                                      autoneg=autoneg)\n",
    "#max_seq_length = 39\n",
    "max_seq_length = X_train1.shape[1]\n",
    "print(\"Max seq length:\", max_seq_length)\n",
    "print(\"X_train:\", X_train1.shape)\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "print(\"X_test:\", X_test1.shape)\n",
    "print(\"Y_test:\", Y_test.shape)\n",
    "\n",
    "# Loading embeddings matrix\n",
    "emb_fn = \"msrpc_{}_{}_{}_{}_{}.pickle\".format(pp_name, lower_opt, emb_name, emb_opt, version)\n",
    "[embedding_matrix, unknown_words] = pickle.load(open(\"./data/\"+emb_fn, 'rb'))\n",
    "embeddings_dim = embedding_matrix.shape[1]\n",
    "print(\"Embeddings dim:\", embeddings_dim)\n",
    "\n",
    "\n",
    "####################\n",
    "# MODEL PARAMETERS #\n",
    "####################\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "window = 3\n",
    "# method = linear or orthogonal\n",
    "method = \"orthogonal\"\n",
    "filters = [(1,500), (2,500), (3,500)]   ####change to 200 \n",
    "use_class_weight = False\n",
    "############################\n",
    "### END MODEL PARAMETERS ###\n",
    "############################\n",
    "\n",
    "# Transforming train data from sequence of indeces to\n",
    "# sequence of embeddings\n",
    "# shape input: (samples, max_seq_length)\n",
    "#      output: (samples, max_seq_length, embeddings_dim)\n",
    "X_train1 = lexdecomp_model.transform_data(X_train1, embedding_matrix)\n",
    "X_train2 = lexdecomp_model.transform_data(X_train2, embedding_matrix)\n",
    "X_test1 = lexdecomp_model.transform_data(X_test1, embedding_matrix)\n",
    "X_test2 = lexdecomp_model.transform_data(X_test2, embedding_matrix)\n",
    "\n",
    "# Decomposing train and test data\n",
    "# shape output: (samples, max_seq_length, embeddings_dim, 2)\n",
    "print(\"Decomposing training data\")\n",
    "X_train1, X_train2 = lexdecomp_model.decompose_data(X_train1, X_train2, window, method)\n",
    "print(\"Decomposing test data\")\n",
    "X_test1, X_test2 = lexdecomp_model.decompose_data(X_test1, X_test2, window, method)\n",
    "print(\"Decomposed data\")\n",
    "print(\"X_train:\", X_train1.shape)\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "print(\"X_test:\", X_test1.shape)\n",
    "print(\"Y_test:\", Y_test.shape)\n",
    "\n",
    "# Selecting the model\n",
    "model = lexdecomp_model.lexdecomp_model((max_seq_length, embeddings_dim, 2),\n",
    "                                        embeddings_dim, max_seq_length, filters)\n",
    "# Printing summaries\n",
    "model.summary(line_length=100)\n",
    "\n",
    "# Compiling model\n",
    "model.compile(optimizer=\"Adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training model\n",
    "# Defining class weights for unbalanced datasets\n",
    "if use_class_weight:\n",
    "    if Y_train[Y_train == 1].size > Y_train[Y_train == 0].size:\n",
    "        class_weight = {1:1.0, 0: Y_train[Y_train == 1].size / Y_train[Y_train == 0].size}\n",
    "    else:\n",
    "        class_weight = {1:Y_train[Y_train == 0].size / Y_train[Y_train == 1].size, 0: 1.0}\n",
    "    print(\"class_weight\", class_weight)\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "# Callback to store prediction scores for each epoch\n",
    "class prediction_history(Callback):\n",
    "    def __init__(self):\n",
    "        self.acchis = []\n",
    "        self.f1his = []\n",
    "        self.cmhis = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pred=self.model.predict([X_test1, X_test2])\n",
    "        predclass = np.where(pred>0.5, 1, 0).reshape(-1)\n",
    "        acc = accuracy_score(Y_test, predclass)\n",
    "        print(acc)\n",
    "        self.acchis.append(acc)\n",
    "        f1 = f1_score(Y_test, predclass)\n",
    "        print(f1)\n",
    "        self.f1his.append(f1)\n",
    "        cm = confusion_matrix(Y_test, predclass)\n",
    "        print(cm)\n",
    "        self.cmhis.append(cm)\n",
    "\n",
    "per_epoch_preds = prediction_history()\n",
    "\n",
    "# Training model\n",
    "print(\"Training model ...\")\n",
    "my_calls = [per_epoch_preds]#None#[es]\n",
    "\n",
    "history = model.fit(x=[X_train1, X_train2],\n",
    "                    y=Y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    #validation_split=0.1,\n",
    "                    validation_data=([X_test1, X_test2], Y_test),\n",
    "                    class_weight=class_weight,\n",
    "                    callbacks=my_calls)\n",
    "\n",
    "\n",
    "print(\"Evaluation (loss, acc)\")\n",
    "loss, acc = model.evaluate(x=[X_test1, X_test2], y=Y_test)\n",
    "print(\"loss: {:.4f}   acc: {:.4f}\".format(loss, acc))\n",
    "with open(\"tmp.p\", \"wb\") as fid:\n",
    "    pickle.dump(model.history.history, fid)\n",
    "pred = np.where(model.predict(x=[X_test1, X_test2])>0.5, 1, 0).reshape(-1)\n",
    "f1 = f1_score(Y_test, pred)\n",
    "print(\"f1: {:.4f}\".format(f1))\n",
    "print(\"confusion matrix\")\n",
    "cf_mat = confusion_matrix(Y_test, pred)\n",
    "print(cf_mat)\n",
    "history.history[\"test_loss\"] = loss\n",
    "history.history[\"test_acc\"] = acc\n",
    "history.history[\"f1\"] = f1\n",
    "history.history[\"cf_mat\"] = cf_mat\n",
    "history.history[\"pred\"] = pred\n",
    "\n",
    "hdir = \"./runs/wang_lexdecomp/\"\n",
    "date = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hfname = hdir + \"hist_\" + date + \"_wang_lexdecopm.p\"\n",
    "with open(hfname, \"wb\") as fid:\n",
    "    pickle.dump(history.history, fid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_sequence(texts, word_to_index, padding=False, size_limit=10):\n",
    "    sequences = {}\n",
    "    for idx, tokens in texts.items():\n",
    "        if padding:\n",
    "            sequences[idx] = np.array([word_to_index[token] for i, token in enumerate(tokens)\n",
    "                                      if i < size_limit] + [0]*(max(0, size_limit-len(tokens))),\n",
    "                                      dtype=np.int32)\n",
    "        else:\n",
    "            sequences[idx] = np.array([word_to_index[token] for token in tokens],\n",
    "                                      dtype=np.int32)\n",
    "    return sequences\n",
    "\n",
    "def generate_dataset(pp_name, lower_opt, version, max_seq_length=-1,\n",
    "            reverse_train_pairs=False, padding=True, autoneg=0):\n",
    "    if padding:\n",
    "        res = generate_dataset_with_padding(pp_name, lower_opt, version, max_seq_length,\n",
    "                reverse_train_pairs, autoneg)\n",
    "    else:\n",
    "        res = generate_dataset_without_padding(pp_name, lower_opt, version, max_seq_length,\n",
    "                reverse_train_pairs, autoneg)\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_dataset_with_padding(pp_name, lower_opt, version, max_seq_length=-1,\n",
    "            reverse_train_pairs=False, autoneg=0):\n",
    "    parsed_fn = \"msrpc_{}_{}_{}.pickle\".format(pp_name, lower_opt, version)\n",
    "\n",
    "    # Loading pre-processed corpus\n",
    "    [parsed_texts,\n",
    "     index_to_word,\n",
    "     word_to_index,\n",
    "     pairs_train,\n",
    "     Y_train_list,\n",
    "     pairs_test,\n",
    "     Y_test_list] = pickle.load(open(\"./data/\"+parsed_fn, 'rb'))\n",
    "\n",
    "    # Computing the max_seq_length if not provided\n",
    "    if max_seq_length < 0:\n",
    "        max_seq_length = np.max([len(tokens) for idx, tokens in parsed_texts.items()])\n",
    "\n",
    "    # Transforming list of tokens to sequence of indices\n",
    "    sequences = convert_to_sequence(parsed_texts, word_to_index,\n",
    "                                    padding=True, size_limit=max_seq_length)\n",
    "\n",
    "    # Training Data\n",
    "    if reverse_train_pairs:\n",
    "        X_train1 = np.zeros((len(pairs_train)*2+autoneg, max_seq_length), dtype=np.int32)\n",
    "        X_train2 = np.zeros((len(pairs_train)*2+autoneg, max_seq_length), dtype=np.int32)\n",
    "        for i, (x1, x2) in enumerate(pairs_train):\n",
    "            X_train1[i*2,:] = sequences[x1]\n",
    "            X_train1[i*2+1,:] = sequences[x2]\n",
    "            X_train2[i*2,:] = sequences[x2]\n",
    "            X_train2[i*2+1,:] = sequences[x1]\n",
    "        Y_train = np.array([Y_train_list[i//2] for i in range(len(Y_train_list)*2)]+[0]*autoneg, dtype=np.int32)\n",
    "    else:\n",
    "        X_train1 = np.zeros((len(pairs_train)+autoneg, max_seq_length), dtype=np.int32)\n",
    "        X_train2 = np.zeros((len(pairs_train)+autoneg, max_seq_length), dtype=np.int32)\n",
    "        for i, (x1, x2) in enumerate(pairs_train):\n",
    "            X_train1[i,:] = sequences[x1]\n",
    "            X_train2[i,:] = sequences[x2]\n",
    "        Y_train = np.array(Y_train_list+[0]*autoneg, dtype=np.int32)\n",
    "\n",
    "    # Adding automatically generated negative samples\n",
    "    # from sentences in positive samples\n",
    "    left, right = zip(*[tup for tup, _class in zip(pairs_train, Y_train_list) if _class==1])\n",
    "    pos_ids = np.array(list(set(left+right)), dtype=np.int32)\n",
    "    selected_pos_ids = np.random.choice(pos_ids, size=autoneg)\n",
    "    pairs_train_set = set(pairs_train)\n",
    "    pairs_test_set = set(pairs_test)\n",
    "    all_ids = np.array(list(parsed_texts.keys()), dtype=np.int32)\n",
    "    starting_i = len(pairs_train)*2 if reverse_train_pairs else len(pairs_train)\n",
    "    for i, pos_id in enumerate(selected_pos_ids, start=starting_i):\n",
    "        while True:\n",
    "            paired_id = np.random.choice(all_ids)\n",
    "            # Check it is not in test set too\n",
    "            if ((pos_id, paired_id) not in pairs_train_set and\n",
    "                   (paired_id, pos_id) not in pairs_train_set and\n",
    "                   (pos_id, paired_id) not in pairs_test_set and\n",
    "                   (paired_id, pos_id) not in pairs_test_set):\n",
    "                X_train1[i,:] = sequences[pos_id]\n",
    "                X_train2[i,:] = sequences[paired_id]\n",
    "                break\n",
    "            else:\n",
    "                print(\"Ignoring randomly generated sample that already exists\")\n",
    "\n",
    "    # Test Data\n",
    "    X_test1 = np.zeros((len(pairs_test), max_seq_length), dtype=np.int32)\n",
    "    X_test2 = np.zeros((len(pairs_test), max_seq_length), dtype=np.int32)\n",
    "    for i, (x1, x2) in enumerate(pairs_test):\n",
    "        X_test1[i,:] = sequences[x1]\n",
    "        X_test2[i,:] = sequences[x2]\n",
    "    Y_test = np.array(Y_test_list, dtype=np.int32)\n",
    "\n",
    "    return index_to_word, word_to_index, X_train1, X_train2, Y_train, X_test1, X_test2, Y_test\n",
    "\n",
    "\n",
    "def generate_dataset_without_padding(pp_name, lower_opt, version, max_seq_length=-1,\n",
    "            reverse_train_pairs=False, autoneg=0):\n",
    "    parsed_fn = \"msrpc_{}_{}_{}.pickle\".format(pp_name, lower_opt, version)\n",
    "\n",
    "    # Loading pre-processed corpus\n",
    "    [parsed_texts,\n",
    "     index_to_word,\n",
    "     word_to_index,\n",
    "     pairs_train,\n",
    "     Y_train_list,\n",
    "     pairs_test,\n",
    "     Y_test_list] = pickle.load(open(\"./data/\"+parsed_fn, 'rb'))\n",
    "\n",
    "    # Computing the max_seq_length if not provided\n",
    "    if max_seq_length < 0:\n",
    "        max_seq_length = np.max([len(tokens) for idx, tokens in parsed_texts.items()])\n",
    "\n",
    "    # Transforming list of tokens to sequence of indices\n",
    "    sequences = convert_to_sequence(parsed_texts, word_to_index,\n",
    "                                    padding=False, size_limit=max_seq_length)\n",
    "\n",
    "    # Training Data\n",
    "    if reverse_train_pairs:\n",
    "        X_train1 = []\n",
    "        X_train2 = []\n",
    "        for i, (x1, x2) in enumerate(pairs_train):\n",
    "            X_train1.append(np.array(sequences[x1], dtype=np.int32))\n",
    "            X_train1.append(np.array(sequences[x2], dtype=np.int32))\n",
    "            X_train2.append(np.array(sequences[x2], dtype=np.int32))\n",
    "            X_train2.append(np.array(sequences[x1], dtype=np.int32))\n",
    "        Y_train = np.array([Y_train_list[i//2] for i in range(len(Y_train_list)*2)]+[0]*autoneg, dtype=np.int32)\n",
    "    else:\n",
    "        X_train1 = []\n",
    "        X_train2 = []\n",
    "        for i, (x1, x2) in enumerate(pairs_train):\n",
    "            X_train1.append(np.array(sequences[x1], dtype=np.int32))\n",
    "            X_train2.append(np.array(sequences[x2], dtype=np.int32))\n",
    "        Y_train = np.array(Y_train_list+[0]*autoneg, dtype=np.int32)\n",
    "\n",
    "    # Adding automatically generated negative samples\n",
    "    # from sentences in positive samples\n",
    "    left, right = zip(*[tup for tup, _class in zip(pairs_train, Y_train_list) if _class==1])\n",
    "    pos_ids = np.array(list(set(left+right)), dtype=np.int32)\n",
    "    selected_pos_ids = np.random.choice(pos_ids, size=autoneg)\n",
    "    pairs_train_set = set(pairs_train)\n",
    "    pairs_test_set = set(pairs_test)\n",
    "    all_ids = np.array(list(parsed_texts.keys()), dtype=np.int32)\n",
    "    starting_i = len(pairs_train)*2 if reverse_train_pairs else len(pairs_train)\n",
    "    for i, pos_id in enumerate(selected_pos_ids, start=starting_i):\n",
    "        while True:\n",
    "            paired_id = np.random.choice(all_ids)\n",
    "            # Check it is not in test set too\n",
    "            if ((pos_id, paired_id) not in pairs_train_set and\n",
    "                   (paired_id, pos_id) not in pairs_train_set and\n",
    "                   (pos_id, paired_id) not in pairs_test_set and\n",
    "                   (paired_id, pos_id) not in pairs_test_set):\n",
    "                X_train1.append(np.array(sequences[pos_id], dtype=np.int32))\n",
    "                X_train2.append(np.array(sequences[paired_id], dtype=np.int32))\n",
    "                break\n",
    "            else:\n",
    "                print(\"Ignoring randomly generated sample that already exists\")\n",
    "\n",
    "    # Test Data\n",
    "    X_test1 = []\n",
    "    X_test2 = []\n",
    "    for i, (x1, x2) in enumerate(pairs_test):\n",
    "        X_test1.append(np.array(sequences[x1], dtype=np.int32))\n",
    "        X_test2.append(np.array(sequences[x2], dtype=np.int32))\n",
    "    Y_test = np.array(Y_test_list, dtype=np.int32)\n",
    "\n",
    "    return (index_to_word, word_to_index,\n",
    "            np.array(X_train1), np.array(X_train2), Y_train,\n",
    "            np.array(X_test1), np.array(X_test2), Y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
